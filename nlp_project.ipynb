{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QHotkQsZ-I4"
      },
      "source": [
        "# DOWNLOADS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "087JL63wQcP7",
        "outputId": "21a202ad-40e4-469e-fe13-7488baa16413"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fde44f093960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install googletrans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    434\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    437\u001b[0m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[1;32m    438\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       p = subprocess.Popen(\n\u001b[0m\u001b[1;32m    185\u001b[0m           \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/bin/bash'"
          ]
        }
      ],
      "source": [
        "pip install googletrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml6P3YpesENk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('tagset')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7inOtAT4aD8D"
      },
      "source": [
        "# Abbreviations - Post tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u__9lYx2uFuu"
      },
      "source": [
        "Abbreviation - Meaning\n",
        "\n",
        "CC- coordinating conjunction \n",
        "\n",
        "CD - cardinal digit\n",
        "\n",
        "DT - determiner\n",
        " \n",
        "EX- existential there \n",
        "\n",
        "FW - foreign word \n",
        "\n",
        "IN - preposition/subordinating conjunction \n",
        "\n",
        "JJ - This NLTK POS Tag is an adjective (large)\n",
        "\n",
        "JJR - adjective, comparative (larger)\n",
        "\n",
        "JJS - adjective, superlative (largest)\n",
        "\n",
        "LS - list market\n",
        "\n",
        "MD - modal (could, will)\n",
        "\n",
        "NN - noun ,singular (cat, tree)\n",
        "\n",
        "NNS - noun plural (desks)\n",
        "\n",
        "NNP - proper noun,singular (sarah)\n",
        "\n",
        "NNPS - proper noun, plural (indians or americans),PDT\tpredeterminer (all, both, half)\n",
        "\n",
        "POS - possessive ending (parent\\ ‘s) , PRP\tpersonal pronoun (hers, herself, him, himself)\n",
        "\n",
        "PRP$ - possessive pronoun (her, his, mine, my, our )\n",
        "\n",
        "RB - adverb (occasionally, swiftly)\n",
        "\n",
        "RBR - adverb ,comparative (greater)\n",
        "\n",
        "RBS - adverb ,superlative (biggest)\n",
        "\n",
        "RP - particle (about)\n",
        "\n",
        "TO - infinite marker (to)\n",
        "\n",
        "UH - interjection (goodbye)\n",
        "\n",
        "VB - verb (ask)\n",
        "\n",
        "VBG - verb gerund (judging)\n",
        "\n",
        "VBD - verb past tense (pleaded)\n",
        "\n",
        "VBN - verb past participle (reunified)\n",
        "\n",
        "VBP - verb , present tense not 3rd person singular(wrap)\n",
        "\n",
        "VBZ - verb ,present tense with 3rd person singular (bases)\n",
        "\n",
        "WDT - wh-determiner (that, what)\n",
        "\n",
        "WP\twh- pronoun (who)\n",
        "\n",
        "WRB - wh- adverb (how)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqZAZLbvaWTy"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foMXwth48C9m"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import tnt\n",
        "from nltk.corpus import indian\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk import pos_tag, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecSaHsULlZYT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKetbthHxisl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFk1zCrxadHV"
      },
      "source": [
        "Loading dataset from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csleY6C1ldUn"
      },
      "outputs": [],
      "source": [
        "data_folder = os.path.join(os.getcwd(),'/content/drive/MyDrive/input')\n",
        "print(data_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_NOQjoUamVx"
      },
      "source": [
        "Loading dataset into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyJhSZHIfzJb"
      },
      "outputs": [],
      "source": [
        "train_text = list()\n",
        "for root,folders,files in os.walk(data_folder):\n",
        "  for file in files:\n",
        "    path=os.path.join(root,file)\n",
        "    with open(path) as inf:\n",
        "      txt = \"\"\n",
        "      for line in inf:\n",
        "        if not line.strip():\n",
        "          continue\n",
        "        d = line.strip()\n",
        "        txt += d\n",
        "      train_text.append(txt)\n",
        "train_df = pd.DataFrame()\n",
        "train_df[\"Text\"] = train_text\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NosDKcNZh2If"
      },
      "outputs": [],
      "source": [
        "print(len(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKd-xaToqte7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "my_punct = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '.', '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_', '`', '{', '}', '~', '»', '«', '“', '”',\"|\"]\n",
        "for i in range(len(train_df)):\n",
        "  txt = train_df._get_value(i, 'Text')\n",
        "  \n",
        "  punct_pattern = re.compile(\"[\" + re.escape(\"\".join(my_punct)) + \"]\") \n",
        "  train_df.at[i,'Text']=  re.sub(punct_pattern, \"\", txt)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LU-p8UAlHLl"
      },
      "outputs": [],
      "source": [
        "hindi_stopwords=pd.read_csv('/content/final_stopwords.txt', delimiter='\\n', header=None)[0].values\n",
        "hindi_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUWRgwj3nlwb"
      },
      "outputs": [],
      "source": [
        "hcount=0\n",
        "l=[]\n",
        "for ans in train_df[\"Text\"]:\n",
        "    tokens=ans.split()\n",
        "    for token in tokens:\n",
        "        if token in hindi_stopwords:\n",
        "            l.append(token)\n",
        "            hcount+=1\n",
        "print(hcount)\n",
        "for i in train_df[\"Text\"]:\n",
        "  print(i,len(i))\n",
        "  break\n",
        "train_df['article'] = train_df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (hindi_stopwords)]))\n",
        "for i in train_df[\"article\"]:\n",
        "  print(i,len(i))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv7hi_74aw0f"
      },
      "source": [
        "Tokenization and Keyword generation(based on regex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50_VFN5D7SKQ"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import tnt\n",
        "from nltk.corpus import indian\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def hindi_model():\n",
        "    train_data = indian.tagged_sents('hindi.pos')\n",
        "    tnt_pos_tagger = tnt.TnT()\n",
        "    tnt_pos_tagger.train(train_data)\n",
        "    return tnt_pos_tagger\n",
        "\n",
        "\n",
        "def get_keywords(pos):\n",
        "    grammar = r\"\"\"NP:{<NN.*>}\"\"\"\n",
        "    #grammar = r\"\"\"NP:{<DT>?<JJ>*<NN>}\"\"\"\n",
        "    chunkParser = nltk.RegexpParser(grammar)\n",
        "    chunked = chunkParser.parse(pos)\n",
        "    continuous_chunk = set()\n",
        "    current_chunk = []\n",
        "    for i in chunked:\n",
        "        if type(i) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.add(named_entity)\n",
        "                current_chunk = []\n",
        "            else:\n",
        "                continue\n",
        "    return (continuous_chunk)\n",
        "\n",
        "\n",
        "\n",
        "for text in train_df['article']:\n",
        "  model = hindi_model()\n",
        "  print(text)\n",
        "  print()\n",
        "  new_tagged = (model.tag(nltk.word_tokenize(text)))\n",
        "  print(set(new_tagged))\n",
        "  print()\n",
        "  print(\"====KEYWORDS===\")\n",
        "  print(get_keywords(new_tagged))\n",
        "  print(\"\\n\")\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PRDRtEbTjJ"
      },
      "source": [
        ":Removing Punctuations from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU7A7_T8zUds"
      },
      "outputs": [],
      "source": [
        "def hindi_model():\n",
        "    train_data = indian.tagged_sents('hindi.pos')\n",
        "    tnt_pos_tagger = tnt.TnT()\n",
        "    tnt_pos_tagger.train(train_data)\n",
        "    return tnt_pos_tagger\n",
        "\n",
        "for text in train_df['Text']:\n",
        "  model = hindi_model()\n",
        "  new_tagged = (model.tag(nltk.word_tokenize(text)))\n",
        "  print(set(new_tagged))\n",
        "  print()\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH1vcN7vzZ9N"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(new_tagged)):\n",
        "  if new_tagged[i][1] == \"PREP\":\n",
        "    print(new_tagged[i][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKNrf9WqiRwL"
      },
      "outputs": [],
      "source": [
        "#train_df.to_csv(\"train_csv.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "douHxNBL1eV5"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import tnt\n",
        "from nltk.corpus import indian\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thWYRex_1N4k"
      },
      "outputs": [],
      "source": [
        "pip install googletrans==4.0.0rc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Qpfx8NbmsS"
      },
      "source": [
        "Handling the Unknown pos tag words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXKfHSBmNkfD"
      },
      "outputs": [],
      "source": [
        "translator = Translator()\n",
        "sentence_id = 0\n",
        "\n",
        "model_path = \"/content/hindi.pos\" \n",
        "\n",
        "def train_hindi_model(model_path):\n",
        "    train_data = indian.tagged_sents(model_path)\n",
        "    tnt_pos_tagger = tnt.TnT()\n",
        "    tnt_pos_tagger.train(train_data)\n",
        "    return tnt_pos_tagger\n",
        "\n",
        "\n",
        "def get_sentId(model_path):\n",
        "    ids = re.compile('<Sentence\\sid=\\d+>')\n",
        "    with open(model_path, \"r+\") as temp_f:\n",
        "        content = temp_f.readlines()\n",
        "        for i in content:\n",
        "            id_found = (ids.findall(i))\n",
        "            if id_found:\n",
        "                id_found = str(id_found).replace(\"['<Sentence id=\", \"\").replace(\">']\", \"\")\n",
        "                id = int(id_found)\n",
        "    id = id + 1\n",
        "    return id\n",
        "\n",
        "\n",
        "def tag_words(model,text):\n",
        "    tagged = (model.tag(nltk.word_tokenize(text)))\n",
        "    return tagged\n",
        "\n",
        "\n",
        "def handle_UNK(tagged_words, model_path, sentence_id):\n",
        "    with open(model_path, \"r+\") as f1:\n",
        "        result_list = []\n",
        "        for nep_word, tag in tagged_words:\n",
        "            if tag == \"Unk\":\n",
        "                x = translator.translate(nep_word)\n",
        "                if x is not None:\n",
        "                    str1 = str(x)\n",
        "                    new_str = str1.split()\n",
        "                    for j in new_str:\n",
        "                        if re.search('^text=', j, re.I):\n",
        "                            word = j.replace(\"text=\", \",\").replace(\",\", \"\")\n",
        "                            word = str(word)\n",
        "                            pos = nltk.tag.pos_tag([word])\n",
        "                            for en_word, tag in pos:\n",
        "                                result = nep_word + \"_\" + (tag) + \" \"\n",
        "                                result_list.append(result)\n",
        "\n",
        "            else:\n",
        "                result = nep_word + \"_\" + (tag) + \" \"\n",
        "                result_list.append(result)\n",
        "\n",
        "        writing_word = str(\"\\n<Sentence id=\") + str(sentence_id) + \">\\n\"\n",
        "        output = writing_word + \"\".join(result_list) + \"\\n</Sentence>\\n</Corpora>\"\n",
        "        for line in f1.readlines():\n",
        "            f1.write(line.replace(\"</Corpora>\", \"\"))\n",
        "        f1.write(output)\n",
        "\n",
        "\n",
        "sentence_id = (get_sentId(model_path))\n",
        "model = train_hindi_model(model_path)\n",
        "for text in train_df['article']:\n",
        "  tagged_words = tag_words(model,text)\n",
        "  \n",
        "  handle_UNK(tagged_words,model_path,sentence_id)\n",
        "\n",
        "#retrain the model\n",
        "model = train_hindi_model(model_path)\n",
        "new_tagged_words =  tag_words(model,text)\n",
        "print (\"=================================New Tagged words=================================\\n\\n\",new_tagged_words,\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7inOtAT4aD8D"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}